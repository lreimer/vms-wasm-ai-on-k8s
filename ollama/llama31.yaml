apiVersion: ollama.ayaka.io/v1
kind: Model
metadata:
  name: llama3.1
spec:
  replicas: 1
  image: llama3.1
  imagePullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 4
      memory: 8Gi
      nvidia.com/gpu: 1 # If you got GPUs
    requests:
      cpu: 4
      memory: 8Gi
      nvidia.com/gpu: 1 # If you got GPUs
  # storageClassName: local-path
  # If you need to specify the access mode for the PersistentVolume
  persistentVolume:
    accessMode: ReadWriteOnce